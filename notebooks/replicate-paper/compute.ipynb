{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for parallel computation of n-Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameter: notebook id\n",
    "aid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second compute wave\n",
    "#aid = aid + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import nshap\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import paperutil\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The different compute jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['folk_income', 'folk_travel', 'housing', 'credit', 'iris']\n",
    "classifiers = ['rf', 'knn', 'gam', 'gbtree']\n",
    "examples = list(range(0, 100))\n",
    "\n",
    "all_jobs = list(product(data_sets, classifiers, examples))\n",
    "print(len(all_jobs), 'different compute jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_set in data_sets:\n",
    "    X_train, X_test, Y_train, Y_test, feature_names = datasets.load_dataset(data_set)\n",
    "    print(data_set, X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The current job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = aid\n",
    "dataset = all_jobs[job_id][0]\n",
    "classifier = all_jobs[job_id][1]\n",
    "example = all_jobs[job_id][2]\n",
    "random_seed = example\n",
    "\n",
    "print(job_id, dataset, classifier, example, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output dir structure, if it does not already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists( f'../../results/n_shapley_values/{dataset}' ):\n",
    "    os.mkdir( f'../../results/n_shapley_values/{dataset}' )\n",
    "if not os.path.exists( f'../../results/n_shapley_values/{dataset}/{classifier}' ):\n",
    "    os.mkdir( f'../../results/n_shapley_values/{dataset}/{classifier}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, feature_names = datasets.load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_classification = datasets.is_classification(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = paperutil.train_classifier(dataset, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_classification:\n",
    "    print( sklearn.metrics.accuracy_score(Y_test, clf.predict(X_test)) )\n",
    "else:\n",
    "    print( sklearn.metrics.mean_squared_error(Y_test, clf.predict(X_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_datapoint = example\n",
    "froot = f'../../results/n_shapley_values/{dataset}/{classifier}/observation_{i_datapoint}'\n",
    "for max_samples in [500, 5000]:\n",
    "    num_samples = min(max_samples, X_train.shape[0])\n",
    "    # the value function\n",
    "    vfunc = nshap.vfunc.interventional_shap(clf.predict, X_train, num_samples=num_samples, random_state=0)\n",
    "    fname = froot + f'_predict_{num_samples}.JSON'\n",
    "    if is_classification:\n",
    "        prediction = int( clf.predict( X_test[i_datapoint, :].reshape((1,-1)) ) )\n",
    "        vfunc = nshap.vfunc.interventional_shap(clf.predict_proba, X_train, num_samples=num_samples, random_state=0, target=prediction)\n",
    "        fname = froot + f'_proba_{num_samples}.JSON'\n",
    "    if classifier == 'gam':\n",
    "        vfunc = nshap.vfunc.interventional_shap(clf.decision_function, X_train, num_samples=num_samples, random_state=0)\n",
    "        fname = froot + f'_decision_{num_samples}.JSON'\n",
    "    # compute and save n-Shapley Values\n",
    "    if not os.path.exists(fname):\n",
    "        n_shapley_values = nshap.n_shapley_values(X_test[i_datapoint, :].reshape((1,-1)), vfunc)\n",
    "        n_shapley_values.save(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
